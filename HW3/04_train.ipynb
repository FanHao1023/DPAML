{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c511cbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio \n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.mobilenet import MobileNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49d5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_matrix(labels, C):\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'.\n",
    "    depth = tf.constant(C, name = \"C\")\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(labels, depth, axis=0)\n",
    "    \n",
    "    # Create the session\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session.\n",
    "    sess.close() \n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c606df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2520, 224, 224, 3)\n",
      "(1, 2520)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "file_path = os.getcwd() + '\\\\train'\n",
    "label_name = os.listdir(file_path)\n",
    "class_num = 3\n",
    "\n",
    "matfn='data_v2.mat' \n",
    "data=sio.loadmat(matfn) \n",
    "\n",
    "\n",
    "X_train = data['X_data'] \n",
    "X_train = X_train / 255\n",
    "Y_train = data['Y_data']\n",
    "# Y_train = Y_train.T\n",
    "\n",
    "# X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "X_val = X_train[4200:]\n",
    "Y_val = Y_train[4200:]\n",
    "\n",
    "X_train = X_train[:4200]\n",
    "Y_train = Y_train[:4200]\n",
    "\"\"\"\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3aca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2520, 3)\n"
     ]
    }
   ],
   "source": [
    "data_num = X_train.shape[0] \n",
    "Y_train = one_hot_matrix(Y_train, class_num)\n",
    "Y_train = Y_train.T\n",
    "Y_train = Y_train.reshape(data_num, class_num)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a387f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "(2520, 3)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69a0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_examples = X_train.shape[0]\n",
    "index_list = list(range(num_examples))\n",
    "random.shuffle(index_list)\n",
    "\n",
    "# 根据打乱后的索引重新排列训练数据和标签\n",
    "X_train = X_train[index_list]\n",
    "Y_train = Y_train[index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b21520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84417ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2016 samples, validate on 504 samples\n",
      "Epoch 1/50\n",
      "2016/2016 [==============================] - 616s 305ms/step - loss: 0.2556 - acc: 0.8983 - val_loss: 10.2337 - val_acc: 0.3651\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.23371, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 2/50\n",
      "2016/2016 [==============================] - 609s 302ms/step - loss: 0.0331 - acc: 0.9876 - val_loss: 9.6385 - val_acc: 0.3750\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.23371 to 9.63847, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 3/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 0.0043 - acc: 0.9980 - val_loss: 4.8029 - val_acc: 0.6647\n",
      "\n",
      "Epoch 00003: val_loss improved from 9.63847 to 4.80294, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 4/50\n",
      "2016/2016 [==============================] - 606s 300ms/step - loss: 0.0071 - acc: 0.9975 - val_loss: 7.2507 - val_acc: 0.4167\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.80294\n",
      "Epoch 5/50\n",
      "2016/2016 [==============================] - 606s 300ms/step - loss: 0.0178 - acc: 0.9955 - val_loss: 9.1248 - val_acc: 0.3651\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.80294\n",
      "Epoch 6/50\n",
      "2016/2016 [==============================] - 608s 302ms/step - loss: 0.0418 - acc: 0.9876 - val_loss: 10.2337 - val_acc: 0.3651\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.80294\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/50\n",
      "2016/2016 [==============================] - 606s 301ms/step - loss: 0.0050 - acc: 0.9990 - val_loss: 6.9845 - val_acc: 0.4345\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.80294\n",
      "Epoch 8/50\n",
      "2016/2016 [==============================] - 606s 300ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 2.2363 - val_acc: 0.6885\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.80294 to 2.23626, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 9/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 9.7794e-04 - acc: 0.9995 - val_loss: 0.3750 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.23626 to 0.37504, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 10/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 0.0044 - acc: 0.9980 - val_loss: 0.9026 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37504\n",
      "Epoch 11/50\n",
      "2016/2016 [==============================] - 610s 302ms/step - loss: 7.9909e-04 - acc: 1.0000 - val_loss: 8.3910e-04 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.37504 to 0.00084, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 12/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 2.8443e-04 - acc: 1.0000 - val_loss: 1.7989e-04 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00084 to 0.00018, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 13/50\n",
      "2016/2016 [==============================] - 608s 302ms/step - loss: 2.0172e-04 - acc: 1.0000 - val_loss: 2.7143e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00018 to 0.00003, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 14/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 1.4882e-04 - acc: 1.0000 - val_loss: 1.7456e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00003 to 0.00002, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 15/50\n",
      "2016/2016 [==============================] - 606s 301ms/step - loss: 1.3409e-04 - acc: 1.0000 - val_loss: 1.2961e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00002 to 0.00001, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 16/50\n",
      "2016/2016 [==============================] - 606s 301ms/step - loss: 5.7799e-05 - acc: 1.0000 - val_loss: 1.1338e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00001 to 0.00001, saving model to MobileNet_nopre_v1.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 17/50\n",
      "2016/2016 [==============================] - 605s 300ms/step - loss: 1.0151e-04 - acc: 1.0000 - val_loss: 9.8501e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00001 to 0.00001, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 18/50\n",
      "2016/2016 [==============================] - 605s 300ms/step - loss: 7.1956e-05 - acc: 1.0000 - val_loss: 9.0654e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00001 to 0.00001, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 19/50\n",
      "2016/2016 [==============================] - 606s 301ms/step - loss: 1.2177e-04 - acc: 1.0000 - val_loss: 9.0338e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00001 to 0.00001, saving model to MobileNet_nopre_v1.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 20/50\n",
      "2016/2016 [==============================] - 608s 301ms/step - loss: 5.4683e-05 - acc: 1.0000 - val_loss: 8.8558e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00001 to 0.00001, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 21/50\n",
      "2016/2016 [==============================] - 606s 301ms/step - loss: 4.6508e-05 - acc: 1.0000 - val_loss: 8.5884e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00001 to 0.00001, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 22/50\n",
      "2016/2016 [==============================] - 609s 302ms/step - loss: 1.2966e-04 - acc: 1.0000 - val_loss: 9.0525e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 23/50\n",
      "2016/2016 [==============================] - 609s 302ms/step - loss: 2.6081e-04 - acc: 1.0000 - val_loss: 8.9183e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00001\n",
      "Epoch 24/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 7.3973e-05 - acc: 1.0000 - val_loss: 8.9590e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00001\n",
      "Epoch 25/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 4.7279e-05 - acc: 1.0000 - val_loss: 8.8951e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 26/50\n",
      "2016/2016 [==============================] - 605s 300ms/step - loss: 2.0631e-04 - acc: 1.0000 - val_loss: 8.8128e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00001\n",
      "Epoch 27/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 5.0313e-05 - acc: 1.0000 - val_loss: 8.7240e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00001\n",
      "Epoch 28/50\n",
      "2016/2016 [==============================] - 608s 301ms/step - loss: 3.7709e-05 - acc: 1.0000 - val_loss: 8.6449e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 29/50\n",
      "2016/2016 [==============================] - 603s 299ms/step - loss: 2.1421e-04 - acc: 1.0000 - val_loss: 8.5520e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00001 to 0.00001, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 30/50\n",
      "2016/2016 [==============================] - 609s 302ms/step - loss: 5.2676e-05 - acc: 1.0000 - val_loss: 8.5460e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00001 to 0.00001, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 31/50\n",
      "2016/2016 [==============================] - 608s 301ms/step - loss: 2.8869e-04 - acc: 1.0000 - val_loss: 8.7345e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 32/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 1.2666e-04 - acc: 1.0000 - val_loss: 8.6911e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00001\n",
      "Epoch 33/50\n",
      "2016/2016 [==============================] - 611s 303ms/step - loss: 2.3679e-05 - acc: 1.0000 - val_loss: 8.8563e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00001\n",
      "Epoch 34/50\n",
      "2016/2016 [==============================] - 608s 302ms/step - loss: 7.0064e-05 - acc: 1.0000 - val_loss: 8.5581e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 35/50\n",
      "2016/2016 [==============================] - 606s 301ms/step - loss: 3.4173e-05 - acc: 1.0000 - val_loss: 8.6476e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00001\n",
      "Epoch 36/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 3.5197e-05 - acc: 1.0000 - val_loss: 8.4886e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00001 to 0.00001, saving model to MobileNet_nopre_v1.h5\n",
      "Epoch 37/50\n",
      "2016/2016 [==============================] - 608s 302ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 8.7504e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 38/50\n",
      "2016/2016 [==============================] - 604s 300ms/step - loss: 4.0736e-05 - acc: 1.0000 - val_loss: 8.7579e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00001\n",
      "Epoch 39/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 8.3484e-05 - acc: 1.0000 - val_loss: 8.7789e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00001\n",
      "Epoch 40/50\n",
      "2016/2016 [==============================] - 601s 298ms/step - loss: 4.9298e-05 - acc: 1.0000 - val_loss: 8.6343e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 41/50\n",
      "2016/2016 [==============================] - 607s 301ms/step - loss: 5.4839e-05 - acc: 1.0000 - val_loss: 8.7756e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00001\n",
      "Epoch 42/50\n",
      "2016/2016 [==============================] - 603s 299ms/step - loss: 4.3758e-05 - acc: 1.0000 - val_loss: 8.9137e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00001\n",
      "Epoch 43/50\n",
      "2016/2016 [==============================] - 616s 306ms/step - loss: 1.4937e-04 - acc: 1.0000 - val_loss: 8.8810e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 44/50\n",
      "2016/2016 [==============================] - 613s 304ms/step - loss: 1.4892e-04 - acc: 1.0000 - val_loss: 8.8733e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00001\n",
      "Epoch 45/50\n",
      "2016/2016 [==============================] - 611s 303ms/step - loss: 8.7040e-05 - acc: 1.0000 - val_loss: 8.8455e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00001\n",
      "Epoch 46/50\n",
      "2016/2016 [==============================] - 609s 302ms/step - loss: 4.2985e-05 - acc: 1.0000 - val_loss: 8.6980e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 47/50\n",
      "2016/2016 [==============================] - 611s 303ms/step - loss: 6.4793e-05 - acc: 1.0000 - val_loss: 8.6963e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00001\n",
      "Epoch 48/50\n",
      "2016/2016 [==============================] - 603s 299ms/step - loss: 8.5524e-05 - acc: 1.0000 - val_loss: 8.9189e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00001\n",
      "Epoch 49/50\n",
      "2016/2016 [==============================] - 993s 493ms/step - loss: 4.1383e-05 - acc: 1.0000 - val_loss: 8.8283e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 50/50\n",
      "2016/2016 [==============================] - 827s 410ms/step - loss: 1.0486e-04 - acc: 1.0000 - val_loss: 8.9874e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00001\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "\n",
    "# early_stopping : val_loss 連續不降低時，結束訓練\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1,restore_best_weights=True,mode='auto')\n",
    "# reduce_lr : val_loss 連續不降低時，減少learning rate\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='auto',\n",
    "                            epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "checkpoint = ModelCheckpoint('MobileNet_nopre_v1.h5', monitor='val_loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "model = MobileNet(input_shape = (224, 224, 3), weights=None, classes=class_num)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x=X_train, y=Y_train, validation_split=0.2, epochs=50, batch_size = 32, callbacks=[early_stopping,checkpoint,reduce_lr])\n",
    "\n",
    "complete_time=time.strftime(\"%Y_%m_%d %H_%M_%S\", time.localtime()) \n",
    "# model.save('MobileNet_'+str(complete_time)+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12c45987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b52d619c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\envs\\MNIST\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.33559003e-05, 9.99976635e-01, 1.15330653e-11],\n",
       "       [1.38452451e-04, 6.05560172e-06, 9.99855518e-01],\n",
       "       [2.52269256e-05, 9.99974370e-01, 3.98961618e-07],\n",
       "       [9.99910474e-01, 8.07157619e-07, 8.86749694e-05],\n",
       "       [3.72142858e-05, 9.99962211e-01, 5.42001430e-07],\n",
       "       [9.06430159e-06, 5.94794528e-06, 9.99984980e-01],\n",
       "       [4.46215236e-05, 9.99946237e-01, 9.18345813e-06],\n",
       "       [1.00000000e+00, 1.28471400e-11, 1.13863159e-08],\n",
       "       [5.49294600e-05, 1.74014131e-04, 9.99771059e-01],\n",
       "       [8.92234129e-06, 6.05132868e-07, 9.99990463e-01],\n",
       "       [6.73754184e-05, 9.99902725e-01, 2.99115200e-05],\n",
       "       [7.72293788e-05, 5.44969982e-04, 9.99377728e-01],\n",
       "       [1.00000000e+00, 1.88632378e-12, 3.61477827e-11],\n",
       "       [1.00000000e+00, 9.67636110e-11, 7.79495690e-10],\n",
       "       [6.91943496e-05, 9.99869347e-01, 6.14040182e-05],\n",
       "       [2.04414828e-05, 5.59002137e-06, 9.99974012e-01],\n",
       "       [3.31604424e-05, 4.30965929e-06, 9.99962568e-01],\n",
       "       [9.99954820e-01, 3.94173139e-09, 4.52356398e-05],\n",
       "       [9.99993563e-01, 3.51223726e-08, 6.49646245e-06],\n",
       "       [1.00954485e-05, 1.55682494e-06, 9.99988317e-01],\n",
       "       [7.73062857e-05, 9.99811113e-01, 1.11585709e-04],\n",
       "       [2.07158519e-06, 9.99997973e-01, 1.72887673e-15],\n",
       "       [1.00000000e+00, 1.33881951e-11, 1.59803779e-08],\n",
       "       [1.11533018e-05, 9.99988794e-01, 3.89608354e-12],\n",
       "       [9.99770343e-01, 1.28618913e-05, 2.16807151e-04],\n",
       "       [8.22865422e-06, 8.61859348e-07, 9.99990940e-01],\n",
       "       [1.00000000e+00, 5.95373195e-10, 1.68046284e-12],\n",
       "       [4.17511319e-06, 9.99995828e-01, 2.31841651e-12],\n",
       "       [3.04111963e-05, 8.60752407e-05, 9.99883533e-01],\n",
       "       [9.68636323e-06, 3.87440286e-06, 9.99986410e-01],\n",
       "       [2.24156211e-05, 1.57531917e-06, 9.99976039e-01],\n",
       "       [1.22738447e-05, 4.90457694e-07, 9.99987245e-01],\n",
       "       [9.99968171e-01, 3.01809977e-09, 3.18845159e-05],\n",
       "       [9.93868744e-05, 3.10406904e-05, 9.99869585e-01],\n",
       "       [1.22457323e-06, 9.99998808e-01, 3.52143275e-15],\n",
       "       [9.99984145e-01, 1.88194127e-09, 1.58056355e-05],\n",
       "       [2.40301451e-05, 1.75894253e-04, 9.99800146e-01],\n",
       "       [1.58110779e-05, 9.99984145e-01, 4.12294747e-11],\n",
       "       [2.56086082e-06, 5.05388982e-07, 9.99996901e-01],\n",
       "       [1.95441044e-05, 9.99980450e-01, 6.15435133e-11],\n",
       "       [5.42862108e-05, 9.99945760e-01, 5.68006697e-10],\n",
       "       [1.61334247e-05, 9.99983907e-01, 5.04444099e-12],\n",
       "       [8.43985072e-06, 2.23642746e-06, 9.99989271e-01],\n",
       "       [1.46459452e-05, 1.24982660e-04, 9.99860406e-01],\n",
       "       [9.99998808e-01, 3.39955869e-10, 1.23843756e-06],\n",
       "       [9.99999642e-01, 4.32403191e-13, 3.39857138e-07],\n",
       "       [5.93519071e-05, 9.99883413e-01, 5.72198187e-05],\n",
       "       [1.24566041e-05, 1.87437035e-06, 9.99985695e-01],\n",
       "       [7.14512571e-05, 1.78633072e-05, 9.99910712e-01],\n",
       "       [9.99989867e-01, 1.23909399e-06, 8.99538554e-06],\n",
       "       [1.00000000e+00, 1.23682713e-11, 1.78958584e-10],\n",
       "       [6.96380039e-06, 3.28205260e-06, 9.99989748e-01],\n",
       "       [9.99907613e-01, 3.43119144e-11, 9.24176056e-05],\n",
       "       [1.00000000e+00, 4.88036206e-13, 1.05973244e-10],\n",
       "       [2.74418790e-06, 9.99997258e-01, 9.20445970e-14],\n",
       "       [4.35325046e-06, 1.87981334e-06, 9.99993801e-01],\n",
       "       [2.53908911e-05, 9.99974608e-01, 1.79295606e-11],\n",
       "       [5.91023163e-05, 9.99925613e-01, 1.52574203e-05],\n",
       "       [2.80770541e-06, 9.99997139e-01, 2.86133998e-14],\n",
       "       [1.00000000e+00, 6.95090235e-11, 6.84761092e-09],\n",
       "       [3.80383353e-05, 9.99961972e-01, 1.11758547e-10],\n",
       "       [1.07647256e-05, 9.99989271e-01, 2.43038228e-12],\n",
       "       [1.32788919e-05, 2.88155047e-06, 9.99983788e-01],\n",
       "       [1.24394610e-05, 2.91692390e-06, 9.99984622e-01],\n",
       "       [7.34444766e-05, 9.99833226e-01, 9.33209885e-05],\n",
       "       [9.99992847e-01, 2.24684507e-11, 7.11537314e-06],\n",
       "       [5.29405179e-06, 3.18659704e-06, 9.99991536e-01],\n",
       "       [6.19855855e-05, 9.99913573e-01, 2.43807608e-05],\n",
       "       [1.30053741e-05, 7.08541211e-06, 9.99979854e-01],\n",
       "       [9.99998689e-01, 3.29451126e-07, 1.00136219e-06],\n",
       "       [2.71907757e-05, 2.27123528e-05, 9.99950051e-01],\n",
       "       [9.99666810e-01, 1.36274105e-06, 3.31846531e-04],\n",
       "       [9.99997497e-01, 6.90485535e-09, 2.52671271e-06],\n",
       "       [1.00132920e-05, 9.99989986e-01, 8.72144913e-13],\n",
       "       [3.51479466e-05, 7.65839941e-05, 9.99888301e-01],\n",
       "       [3.87692444e-06, 3.88697936e-06, 9.99992251e-01],\n",
       "       [9.99998927e-01, 9.13070863e-09, 1.01379726e-06],\n",
       "       [6.23579081e-06, 9.99993801e-01, 1.27615204e-13],\n",
       "       [1.73570865e-04, 1.71627826e-03, 9.98110175e-01],\n",
       "       [4.73031150e-06, 9.99995232e-01, 3.67453091e-14],\n",
       "       [6.62002276e-05, 9.99873042e-01, 6.08088485e-05],\n",
       "       [9.99719799e-01, 1.21886642e-05, 2.68079020e-04],\n",
       "       [9.99659896e-01, 6.60157502e-06, 3.33518750e-04],\n",
       "       [9.99978662e-01, 7.72301612e-10, 2.13359162e-05],\n",
       "       [1.32343666e-05, 9.99986768e-01, 2.75812446e-12],\n",
       "       [1.18980233e-05, 1.59436495e-05, 9.99972105e-01],\n",
       "       [2.58928521e-05, 1.60251686e-04, 9.99813855e-01],\n",
       "       [1.00000000e+00, 2.24559105e-09, 1.16189269e-09],\n",
       "       [4.91227402e-05, 2.44673065e-05, 9.99926448e-01],\n",
       "       [1.93135402e-05, 9.99980688e-01, 3.20083578e-11],\n",
       "       [3.84889136e-04, 9.99615073e-01, 4.72727102e-09],\n",
       "       [1.73884491e-05, 5.54616099e-06, 9.99977112e-01],\n",
       "       [4.47265229e-05, 2.17750622e-03, 9.97777760e-01],\n",
       "       [4.16008625e-05, 9.99957323e-01, 1.05569302e-06],\n",
       "       [2.68520898e-06, 9.99997258e-01, 1.49782242e-14],\n",
       "       [2.15926793e-05, 9.99978304e-01, 1.46329555e-07],\n",
       "       [6.82867394e-05, 1.02781993e-03, 9.98903871e-01],\n",
       "       [9.99999762e-01, 2.20336985e-10, 2.91717527e-07],\n",
       "       [9.99988914e-01, 1.10573392e-05, 2.63163979e-09],\n",
       "       [9.97428238e-01, 2.57170456e-03, 3.11858166e-08]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import models\n",
    "name='ResNet50_prepro_v2.h5'\n",
    "model = models.load_model(name)\n",
    "model.predict(X_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9d532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
